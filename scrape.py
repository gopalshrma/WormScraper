import bs4 as bs
import urllib.request
import sys
import os
from collections import OrderedDict

# Dictionary to store sitemap.
ScraperURL = []

# Reads which web series to scrape.
# Changes input to lower case for ease of use.
option = sys.argv[1].lower()

if len(sys.argv) > 2:
    # The directory to store the output file in.
    directory = sys.argv[2]
else:
    # If no directory is supplied default to the current directory.
    directory = os.getcwd()

validdir = os.path.isdir(directory)

if validdir is False:
    print("Invalid directory. Enter a valid directory.")

while option not in ['worm','pact','twig']:
    print("I'm not familiar with that particular web series. Current support is for worm, pact, and twig.\n")
    print("Please specify one of the works that are supported or use Ctrl + c to terminate.\n")
# Turn all inputs to lower case.
    option = input().lower()
# Runs the prerequisite python file which builds the sitemap.
# This can be done manually if needed.
try:
    os.system("python3 sitemap/" + option + ".py")
except Exception as exception:
    print(exception)
# Reads the sitemap data generated by the previous system call.
with open(option + '-sitemap.txt') as f:

# Loads data into dictionary and splits it by new line
# Each entry in the dictionary is now a seperate URL to be scraped.
    ScraperURL = f.read()
    ScraperURL = ScraperURL.split("\n")

"""

Turns out some anchor tags are repeated for no reason at all (Atleast no real reason that I can think of)
in both worm's and pact's table of contents, so I could either rescrape from sitemap which would make
the entire thing flow in reverse order because sitemap updates as story updates, or remove
duplicates from the list while preserving order. Which is what this piece of code does.

For future reference :

https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-in-whilst-preserving-order

"""
ScraperURL = list(OrderedDict.fromkeys(ScraperURL))

# Opens the file the output is to be written in.
file = open(directory + "/" + option + ".html","w",encoding="utf-8")

# Run scraper for every entry in the dictionary.
for url in ScraperURL:
    SourceURL = url

# Open the url and read the source.
    source = urllib.request.urlopen(SourceURL).read()
    soup = bs.BeautifulSoup(source,'lxml')

# Prints current URL being scraped for debugging convenience.
    print("Starting "+ SourceURL +"\n")


###   Calibre detects titles and chapters inside h1 or h2 tags if they belong to the "chapter" class.
###   This ensures that caibre generates a page break during conversion.

    file.write("<h2 class=\"chapter\">" + soup.title.text + "</h2>")
    file.write("</br></br>")

# For every single p tag, the text within is scraped and added to the current output file.
    for paragraph in soup.find_all('p'):
# Some p tags are used in the source site for paragraph breaks. Ignores those.
        if(paragraph.string!=None and paragraph.string):
# Writes the text to output file.
            paragh = (paragraph.string)
# Avoiding a few words by making sure they're not the current string extracted.
            listtoavoid = ["Next Chapter","Connecting to %s","<strike>","Fill in your details below"]
            if any(lta in paragh for lta in listtoavoid):
                print("Avoiding word.")
            else:
                file.write("<p>" + paragh + "</p>")
        else:
            continue
#End of chapter shows in console output for user convenience.
    print("Done with Chapter\n")
print("All done with " + option + ". Use Calibre to convert into preferred format. :]\n")
print("Removing sitemap files now.")
os.system("rm " + option + "-sitemap.txt")
file.close()
